2023-11-15 22:57:16,968 transreid INFO: Saving model in the path :./output/tiger/vit_ori
2023-11-15 22:57:16,968 transreid INFO: Namespace(config_file='configs/tiger/vit_clipreid_ori.yml', local_rank=0, opts=[])
2023-11-15 22:57:16,968 transreid INFO: Loaded configuration file configs/tiger/vit_clipreid_ori.yml
2023-11-15 22:57:16,968 transreid INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'ViT-B-16'
  STRIDE_SIZE: [16, 16]
  ID_LOSS_WEIGHT : 0.25
  TRIPLET_LOSS_WEIGHT : 1.0
  I2T_LOSS_WEIGHT : 1.0
  # SIE_CAMERA: True
  # SIE_COE : 1.0

INPUT:
  SIZE_TRAIN: [256, 128]
  SIZE_TEST: [256, 128]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.5 # random erasing
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  STAGE1:
    IMS_PER_BATCH: 64
    OPTIMIZER_NAME: "Adam"
    BASE_LR: 0.00035
    WARMUP_LR_INIT: 0.00001
    LR_MIN: 1e-6
    WARMUP_METHOD: 'linear'
    WEIGHT_DECAY:  1e-4
    WEIGHT_DECAY_BIAS: 1e-4
    MAX_EPOCHS: 2 #120
    CHECKPOINT_PERIOD: 2 #120
    LOG_PERIOD: 2 #50
    WARMUP_EPOCHS: 5
  
  STAGE2:
    IMS_PER_BATCH: 64
    OPTIMIZER_NAME: "Adam"
    BASE_LR: 0.000005
    WARMUP_METHOD: 'linear'
    WARMUP_ITERS: 10
    WARMUP_FACTOR: 0.1
    WEIGHT_DECAY:  0.0001
    WEIGHT_DECAY_BIAS: 0.0001
    LARGE_FC_LR: False
    MAX_EPOCHS: 2 #60
    CHECKPOINT_PERIOD: 2 #60
    LOG_PERIOD: 2 #50
    EVAL_PERIOD: 60
    BIAS_LR_FACTOR: 2
    
    STEPS: [30, 50]
    GAMMA: 0.1
  
TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'

DATASETS:
  NAMES: ('animals')
  ROOT_DIR: ('../data/')
  TEST_ROOT_DIR: ('../data/tiger/test_original')
  SPECIES: ('tiger')
  DATA_DIR: ('Animal-2')
OUTPUT_DIR: './output/tiger/vit_ori'


# CUDA_VISIBLE_DEVICES=0 python train_clipreid.py --config_file configs/tiger/vit_clipreid.yml
# CUDA_VISIBLE_DEVICES=0 python test_clipreid.py --config_file configs/tiger/vit_clipreid.yml TEST.WEIGHT './output/tiger/vit/ViT-B-16_1.pth'
# CUDA_VISIBLE_DEVICES=0 python test_rerank.py --config_file configs/tiger/vit_clipreid.yml TEST.WEIGHT './output/tiger/vit/ViT-B-16_1.pth'

2023-11-15 22:57:16,968 transreid INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax_triplet
DATASETS:
  DATA_DIR: Animal-2
  NAMES: animals
  ROOT_DIR: ../data/
  SPECIES: tiger
  TEST_ROOT_DIR: ../data/tiger/test_original
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
MODEL:
  ATT_DROP_RATE: 0.0
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  I2T_LOSS_WEIGHT: 1.0
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  METRIC_LOSS_TYPE: triplet
  NAME: ViT-B-16
  NECK: bnneck
  NO_MARGIN: False
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: 
  SIE_CAMERA: False
  SIE_COE: 3.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: None
  TRIPLET_LOSS_WEIGHT: 1.0
OUTPUT_DIR: ./output/tiger/vit_ori
SOLVER:
  MARGIN: 0.3
  SEED: 1234
  STAGE1:
    BASE_LR: 0.00035
    CHECKPOINT_PERIOD: 2
    COSINE_MARGIN: 0.5
    COSINE_SCALE: 30
    EVAL_PERIOD: 10
    IMS_PER_BATCH: 64
    LOG_PERIOD: 2
    LR_MIN: 1e-06
    MAX_EPOCHS: 2
    MOMENTUM: 0.9
    OPTIMIZER_NAME: Adam
    WARMUP_EPOCHS: 5
    WARMUP_FACTOR: 0.01
    WARMUP_ITERS: 500
    WARMUP_LR_INIT: 1e-05
    WARMUP_METHOD: linear
    WEIGHT_DECAY: 0.0001
    WEIGHT_DECAY_BIAS: 0.0001
  STAGE2:
    BASE_LR: 5e-06
    BIAS_LR_FACTOR: 2
    CENTER_LOSS_WEIGHT: 0.0005
    CENTER_LR: 0.5
    CHECKPOINT_PERIOD: 2
    COSINE_MARGIN: 0.5
    COSINE_SCALE: 30
    EVAL_PERIOD: 60
    GAMMA: 0.1
    IMS_PER_BATCH: 64
    LARGE_FC_LR: False
    LOG_PERIOD: 2
    LR_MIN: 1.6e-05
    MAX_EPOCHS: 2
    MOMENTUM: 0.9
    OPTIMIZER_NAME: Adam
    STEPS: (30, 50)
    WARMUP_EPOCHS: 5
    WARMUP_FACTOR: 0.1
    WARMUP_ITERS: 10
    WARMUP_LR_INIT: 0.01
    WARMUP_METHOD: linear
    WEIGHT_DECAY: 0.0001
    WEIGHT_DECAY_BIAS: 0.0001
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  IMS_PER_BATCH: 64
  NECK_FEAT: before
  RE_RANKING: False
  WEIGHT: 
2023-11-15 22:57:24,147 transreid.train INFO: start training
2023-11-15 22:57:24,161 transreid.train INFO: model: build_transformer(
  (classifier): Linear(in_features=768, out_features=107, bias=False)
  (classifier_proj): Linear(in_features=512, out_features=107, bias=False)
  (bottleneck): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bottleneck_proj): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (image_encoder): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (prompt_learner): PromptLearner()
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
2023-11-15 22:57:41,964 transreid.train INFO: Epoch[1] Iteration[2/27] Loss: 12.565, Base Lr: 7.80e-05
2023-11-15 22:57:42,196 transreid.train INFO: Epoch[1] Iteration[4/27] Loss: 12.316, Base Lr: 7.80e-05
2023-11-15 22:57:42,434 transreid.train INFO: Epoch[1] Iteration[6/27] Loss: 12.083, Base Lr: 7.80e-05
2023-11-15 22:57:42,668 transreid.train INFO: Epoch[1] Iteration[8/27] Loss: 11.700, Base Lr: 7.80e-05
2023-11-15 22:57:42,903 transreid.train INFO: Epoch[1] Iteration[10/27] Loss: 11.572, Base Lr: 7.80e-05
2023-11-15 22:57:43,137 transreid.train INFO: Epoch[1] Iteration[12/27] Loss: 11.247, Base Lr: 7.80e-05
2023-11-15 22:57:43,373 transreid.train INFO: Epoch[1] Iteration[14/27] Loss: 10.998, Base Lr: 7.80e-05
2023-11-15 22:57:43,608 transreid.train INFO: Epoch[1] Iteration[16/27] Loss: 10.733, Base Lr: 7.80e-05
2023-11-15 22:57:43,841 transreid.train INFO: Epoch[1] Iteration[18/27] Loss: 10.561, Base Lr: 7.80e-05
2023-11-15 22:57:44,074 transreid.train INFO: Epoch[1] Iteration[20/27] Loss: 10.430, Base Lr: 7.80e-05
2023-11-15 22:57:44,311 transreid.train INFO: Epoch[1] Iteration[22/27] Loss: 10.258, Base Lr: 7.80e-05
2023-11-15 22:57:44,546 transreid.train INFO: Epoch[1] Iteration[24/27] Loss: 10.134, Base Lr: 7.80e-05
2023-11-15 22:57:44,780 transreid.train INFO: Epoch[1] Iteration[26/27] Loss: 10.052, Base Lr: 7.80e-05
2023-11-15 22:57:45,268 transreid.train INFO: Epoch[2] Iteration[2/27] Loss: 7.737, Base Lr: 1.46e-04
2023-11-15 22:57:45,501 transreid.train INFO: Epoch[2] Iteration[4/27] Loss: 7.686, Base Lr: 1.46e-04
2023-11-15 22:57:45,733 transreid.train INFO: Epoch[2] Iteration[6/27] Loss: 7.726, Base Lr: 1.46e-04
2023-11-15 22:57:45,969 transreid.train INFO: Epoch[2] Iteration[8/27] Loss: 7.592, Base Lr: 1.46e-04
2023-11-15 22:57:46,204 transreid.train INFO: Epoch[2] Iteration[10/27] Loss: 7.489, Base Lr: 1.46e-04
2023-11-15 22:57:46,438 transreid.train INFO: Epoch[2] Iteration[12/27] Loss: 7.401, Base Lr: 1.46e-04
2023-11-15 22:57:46,672 transreid.train INFO: Epoch[2] Iteration[14/27] Loss: 7.326, Base Lr: 1.46e-04
2023-11-15 22:57:46,909 transreid.train INFO: Epoch[2] Iteration[16/27] Loss: 7.212, Base Lr: 1.46e-04
2023-11-15 22:57:47,143 transreid.train INFO: Epoch[2] Iteration[18/27] Loss: 7.141, Base Lr: 1.46e-04
2023-11-15 22:57:47,377 transreid.train INFO: Epoch[2] Iteration[20/27] Loss: 7.079, Base Lr: 1.46e-04
2023-11-15 22:57:47,611 transreid.train INFO: Epoch[2] Iteration[22/27] Loss: 7.024, Base Lr: 1.46e-04
2023-11-15 22:57:47,847 transreid.train INFO: Epoch[2] Iteration[24/27] Loss: 6.938, Base Lr: 1.46e-04
2023-11-15 22:57:48,081 transreid.train INFO: Epoch[2] Iteration[26/27] Loss: 6.835, Base Lr: 1.46e-04
2023-11-15 22:57:48,649 transreid.train INFO: Stage1 running time: 0:00:24.488611
2023-11-15 22:57:48,660 transreid.train INFO: start training
2023-11-15 22:57:59,677 transreid.train INFO: Epoch[1] Iteration[2/25] Loss: 17.103, Acc: 0.086, Base Lr: 9.50e-07
2023-11-15 22:58:00,560 transreid.train INFO: Epoch[1] Iteration[4/25] Loss: 18.775, Acc: 0.066, Base Lr: 9.50e-07
2023-11-15 22:58:01,312 transreid.train INFO: Epoch[1] Iteration[6/25] Loss: 19.113, Acc: 0.091, Base Lr: 9.50e-07
2023-11-15 22:58:02,092 transreid.train INFO: Epoch[1] Iteration[8/25] Loss: 19.290, Acc: 0.102, Base Lr: 9.50e-07
2023-11-15 22:58:02,874 transreid.train INFO: Epoch[1] Iteration[10/25] Loss: 19.123, Acc: 0.109, Base Lr: 9.50e-07
2023-11-15 22:58:03,651 transreid.train INFO: Epoch[1] Iteration[12/25] Loss: 18.381, Acc: 0.117, Base Lr: 9.50e-07
2023-11-15 22:58:04,428 transreid.train INFO: Epoch[1] Iteration[14/25] Loss: 17.619, Acc: 0.118, Base Lr: 9.50e-07
2023-11-15 22:58:05,203 transreid.train INFO: Epoch[1] Iteration[16/25] Loss: 17.077, Acc: 0.112, Base Lr: 9.50e-07
2023-11-15 22:58:05,978 transreid.train INFO: Epoch[1] Iteration[18/25] Loss: 16.496, Acc: 0.107, Base Lr: 9.50e-07
2023-11-15 22:58:06,757 transreid.train INFO: Epoch[1] Iteration[20/25] Loss: 15.973, Acc: 0.110, Base Lr: 9.50e-07
2023-11-15 22:58:07,070 transreid.train INFO: Epoch 1 done. Time per batch: 0.917[s] Speed: 69.8[samples/s]
2023-11-15 22:58:15,577 transreid.train INFO: Epoch[2] Iteration[2/25] Loss: 11.219, Acc: 0.070, Base Lr: 1.40e-06
2023-11-15 22:58:17,439 transreid.train INFO: Epoch[2] Iteration[4/25] Loss: 10.376, Acc: 0.086, Base Lr: 1.40e-06
2023-11-15 22:58:18,709 transreid.train INFO: Epoch[2] Iteration[6/25] Loss: 10.086, Acc: 0.104, Base Lr: 1.40e-06
2023-11-15 22:58:19,490 transreid.train INFO: Epoch[2] Iteration[8/25] Loss: 9.842, Acc: 0.123, Base Lr: 1.40e-06
2023-11-15 22:58:20,280 transreid.train INFO: Epoch[2] Iteration[10/25] Loss: 9.675, Acc: 0.148, Base Lr: 1.40e-06
2023-11-15 22:58:21,050 transreid.train INFO: Epoch[2] Iteration[12/25] Loss: 9.587, Acc: 0.150, Base Lr: 1.40e-06
2023-11-15 22:58:21,822 transreid.train INFO: Epoch[2] Iteration[14/25] Loss: 9.518, Acc: 0.163, Base Lr: 1.40e-06
2023-11-15 22:58:22,598 transreid.train INFO: Epoch[2] Iteration[16/25] Loss: 9.576, Acc: 0.157, Base Lr: 1.40e-06
2023-11-15 22:58:23,381 transreid.train INFO: Epoch[2] Iteration[18/25] Loss: 9.536, Acc: 0.169, Base Lr: 1.40e-06
2023-11-15 22:58:24,150 transreid.train INFO: Epoch[2] Iteration[20/25] Loss: 9.543, Acc: 0.170, Base Lr: 1.40e-06
2023-11-15 22:58:24,484 transreid.train INFO: Epoch 2 done. Time per batch: 0.871[s] Speed: 73.5[samples/s]
2023-11-15 22:58:24,970 transreid.train INFO: Total running time: 0:00:36.306783
